{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b26998c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e07eba8",
   "metadata": {},
   "source": [
    "## Lesson 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae5ccc5",
   "metadata": {},
   "source": [
    "1\\. Describe in writing what an assumption is made when a naive Bayes classifier is created. Why the classifier is naive?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70a8fc7",
   "metadata": {},
   "source": [
    "Assumption in naive Bayes classifier implies that objects in data is independent, that means that $p(F_{i} | C, F_{j}) = p(F_{i} | C)$ for i != j.\n",
    "\n",
    "From this it is esay to understand why it is naive, because objects can not be fully independent and as a result this algo is called 'naive', but it works, so let it be used :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea9760c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62c96ecc",
   "metadata": {},
   "source": [
    "3\\. Make a copy of a naive Bayes classifier that we used above to create a spam filter and try to improve its performance.\n",
    "Split the data set into training, validation and test data. Select the best model using the validation dataset and then compute your final score on the testing data. To improve the model for example the whole message content can be taken into account instead of the subject only. Also lengths of tokens that are taken into account can be varied. May be it would be interesting to split the messages into digramms: couples of words going one after another. And so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63ce9495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "class NaiveBayes:\n",
    "    def __init__(self, k, drop_short):\n",
    "        \"\"\"\n",
    "        k - pseudocount, usually 1\n",
    "        drop_short - drop too short tokens\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.vocab = set()  # vocabulary, i.e., set of all seen tokens\n",
    "        self.token_in_spam = defaultdict(int)   # counters of tokesn in spam\n",
    "        self.token_in_ham  = defaultdict(int)    # ... and in ham messages\n",
    "        self.pcond_spam    = self.pcond_ham = None # conditional probabilities of tokesn, will be computed after training\n",
    "        self.spam_total    = self.ham_total = 0    # total number of spam and ham messages\n",
    "        self.p_spam_total  = self.p_ham_total = None  # marginal probailities of spam and ham messages\n",
    "        self.re_token      = re.compile(r\"[a-z']+\")  # regex to extarct tokens\n",
    "        self.drop_short    = drop_short  # lengths of short tokens to drop out\n",
    "        \n",
    "    def _text2tokens(self, text):\n",
    "        \"\"\"Convert a text to a list of tokens. \n",
    "        We take just the first line of a message that contains a word Subject\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        s          = text_lower.splitlines()[0]\n",
    "        text_lower = s.replace('Subject: ', '')\n",
    "        all_tokens = self.re_token.findall(text_lower)\n",
    "        unique_tokes = list(set(all_tokens))\n",
    "        good_tokens  = [tok for tok in unique_tokes if len(tok) > self.drop_short]\n",
    "        return good_tokens\n",
    "    \n",
    "    def fit(self, messages, labels):\n",
    "        \"\"\"Training: computing the probailities for each token \n",
    "        to be enoucontered in spam and ham messages.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Count tokens in spam and in ham messages\n",
    "        for mes, lab in zip(messages, labels):\n",
    "            tokens = self._text2tokens(mes)\n",
    "            if lab == 'spam':\n",
    "                self.spam_total += 1\n",
    "                for tok in tokens:\n",
    "                    self.token_in_spam[tok] += 1\n",
    "            else:\n",
    "                self.ham_total += 1\n",
    "                for tok in tokens:\n",
    "                    self.token_in_ham[tok] += 1\n",
    "            self.vocab.update(tokens)\n",
    "\n",
    "        # Compute probabilities\n",
    "        self.pcond_spam = defaultdict(int)\n",
    "        self.pcond_ham = defaultdict(int)\n",
    "        for tok in self.vocab:\n",
    "            self.pcond_spam[tok] = (self.token_in_spam[tok] + self.k) / (self.spam_total + 2 * self.k)\n",
    "            self.pcond_ham[tok]  = (self.token_in_ham[tok] + self.k) / (self.ham_total + 2 * self.k)\n",
    "        self.p_spam_total = self.spam_total / (self.spam_total + self.ham_total)\n",
    "        self.p_ham_total  = 1 - self.p_spam_total\n",
    "        \n",
    "    def predict(self, messages):\n",
    "        \"\"\"Prediction: computing labels for messages.\n",
    "        \"\"\"\n",
    "        pred = []\n",
    "        for mes in messages:\n",
    "            message_tokens = self._text2tokens(mes)\n",
    "            log_sum_spam   = np.log(self.p_spam_total)  # collect probailities for spam \n",
    "            log_sum_ham    = np.log(self.p_ham_total)    # ... and ham messages\n",
    "            for tok in self.vocab:\n",
    "                p_spam = self.pcond_spam[tok] \n",
    "                p_ham  = self.pcond_ham[tok]\n",
    "                if tok not in message_tokens:  # if the token absent in the message we take complememnt probailities\n",
    "                    p_spam = 1 - p_spam\n",
    "                    p_ham  = 1 - p_ham\n",
    "                log_sum_spam += np.log(p_spam)\n",
    "                log_sum_ham  += np.log(p_ham)\n",
    "            # Make a desision, spam or ham\n",
    "            pred.append('spam' if log_sum_spam > log_sum_ham else 'ham')\n",
    "        return pred\n",
    "    \n",
    "    def explore_vocab(self):\n",
    "        \"\"\"Make a predicition for every token separately to see\n",
    "        how they influnce the prediction.\n",
    "        \"\"\"\n",
    "        spam_words = []\n",
    "        for tok in self.vocab:\n",
    "            p_spam = self.pcond_spam[tok] * self.p_spam_total\n",
    "            p_ham = self.pcond_ham[tok] * self.p_ham_total\n",
    "            if p_spam > p_ham:\n",
    "                spam_words.append([tok, p_spam])\n",
    "                \n",
    "        spam_words = sorted(spam_words, key=lambda x: -x[1])\n",
    "        words_only = [s[0] for s in spam_words]\n",
    "        return words_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc66f3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download spam_and_ham.zip, unzip spam_ham_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from io import BytesIO, TextIOWrapper\n",
    "from zipfile import ZipFile\n",
    "\n",
    "def load_zipcsv_categorical(file_name):\n",
    "    \"\"\"Downloads zipped csv dataset from repo and return it as a nested list.\"\"\"\n",
    "    base_url = \"https://raw.githubusercontent.com/kupav/data-sc-intro/main/data/\"\n",
    "    web_data = requests.get(base_url + file_name)\n",
    "    assert web_data.status_code == 200\n",
    "\n",
    "    # unzip the content\n",
    "    zf = ZipFile(BytesIO(web_data.content))\n",
    "    \n",
    "    # zipped file name\n",
    "    zipped_name = zf.namelist()[0]\n",
    "    print(f\"Download {file_name}, unzip {zipped_name}\")\n",
    "    \n",
    "    # Open unpacked file\n",
    "    with zf.open(zipped_name, 'r') as file:\n",
    "        # TextIOWrapper(file) converts byte strings to plain strings\n",
    "        reader = csv.reader(TextIOWrapper(file), delimiter=',')\n",
    "        data = []\n",
    "        for row in reader:\n",
    "            data.append(row)\n",
    "    return data\n",
    "\n",
    "raw_data = load_zipcsv_categorical(\"spam_and_ham.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a2b2d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lab = [row[1] for row in raw_data[1:]]\n",
    "data_mes = [row[2] for row in raw_data[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef7c859b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size 4654\n",
      " test size 517\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "p_test = 0.1\n",
    "n_test = round(p_test * len(data_lab))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_mes, data_lab, random_state=0, \n",
    "                                                    test_size=n_test, shuffle=True)\n",
    "\n",
    "print(f\"train size {len(y_train)}\")\n",
    "print(f\" test size {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b848835f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbc = NaiveBayes(k=1, drop_short=2)\n",
    "nbc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d07e1864",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nbc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ff53313e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8627\n",
      "F1-score = 0.7149\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "acc = metrics.accuracy_score(y_test, y_pred)\n",
    "f1 = metrics.f1_score(y_test, y_pred, average='binary', pos_label='spam')\n",
    "\n",
    "print(f\"Accuracy = {acc:.4f}\")\n",
    "print(f\"F1-score = {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fa21f8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.8900\n",
      "Recall    = 0.5973\n",
      "F1-score  = 0.7149\n"
     ]
    }
   ],
   "source": [
    "prec, rec, f1, _ = metrics.precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label='spam')\n",
    "\n",
    "print(f\"Precision = {prec:.4f}\")\n",
    "print(f\"Recall    = {rec:.4f}\")\n",
    "print(f\"F1-score  = {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d69ba69",
   "metadata": {},
   "source": [
    "initial scores:\n",
    "|          |       |\n",
    "|----------|-------|\n",
    "|Accuracy  | 0.8627|\n",
    "|F1-score  | 0.7149|\n",
    "|Precision | 0.8900|\n",
    "|Recall    | 0.5973|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c448e1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I couldn't improve the accuracy, so I'll do another task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e21f895",
   "metadata": {},
   "source": [
    "5\\. Previously we discussed that in the most cases data must be standardized before creation of a machine learning model. Why it does not influences the performance of a Gaussian naive Bayes classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2d2824",
   "metadata": {},
   "source": [
    "outliers (vibros) are filtered out by the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2856c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08b649b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
