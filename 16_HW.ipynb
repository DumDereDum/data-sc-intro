{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d66097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8b5ca15",
   "metadata": {},
   "source": [
    "1\\. Describe in writing why stopword removal is required when a vectorized model of a text is prepared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cbaf6a",
   "metadata": {},
   "source": [
    "Stop words are words without information value in any human language. By removing these words, we remove the low-level information from our text in order to give more focus to the important information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a69d567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "896354fb",
   "metadata": {},
   "source": [
    "2\\. Describe in writing what are stemming and lemmatization. For what purpose they are leveraged?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b9bab2",
   "metadata": {},
   "source": [
    "We use Stemming to remove suffixes from words and end up with a so-called word stem (root by other words). The words “likes”, “likely” and “liked”, for example, all result in their common word stem “like” which can be used as a synonym for all three words. That way, an NLP model can learn that all three words are somehow similar and are used in a similar context.\n",
    "\n",
    "Lemmatization is a development of Stemming and describes the process of grouping together the different inflected forms of a word so they can be analyzed as a single item. Lemmatization is similar to Stemming but it brings context to the words. So it links words with similar meanings to one word. Lemmatization algorithms usually also use positional arguments as inputs, such as whether the word is an adjective, noun, or verb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8dd9d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df0b03a0",
   "metadata": {},
   "source": [
    "4\\. Describe in writing the key differences between BoW and TF-IDF models of text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a03bfc4",
   "metadata": {},
   "source": [
    "- Bag of Words: Converting words to numbers with no semantic information.\n",
    "\n",
    "- TF-IDF: It is also converting the words to numbers or vectors with some weighted information.\n",
    "\n",
    "Technically BOW includes all the methods where words are considered as a set, i.e. without taking order into account. Thus TFIDF belongs to BOW methods: TFIDF is a weighting scheme applied to words considered as a set. There can be many other options for weighting the words in a set.\n",
    "\n",
    "Compared to regular TF-weighted BOW, the TFIDF weighting scheme gives more weight to words which appear in fewer documents and less weight to words which appear in many documents. The rationale is that a word which appears in many documents is unlikely to be relevant since it doesn't help selecting the most similar document. Typically the most frequent words are grammatical words (also called stop words, e.g. determiners, pronouns, etc.), but in a corpus made of sci-fi books for example some words such as \"robot\" or \"planet\" will also be very common. On the contrary a word like \"elephant\" would be very rare in a sci-fi context, so it is given more weight because it's more discriminative.\n",
    "\n",
    "This is meaningful in Information Retrieval tasks where the goal is to find a document similar to a query, and by extension it's useful in most tasks where the goal is to compare text documents by their semantic similarity. It is not meaningful and often counter-productive in classification tasks related to the style of text, as opposed to its semantic content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152aae90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c031bb2e",
   "metadata": {},
   "source": [
    "5\\. Describe in writing what is an idea of word embedding. What are its advantages in comparison with other vectorization techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38708c4",
   "metadata": {},
   "source": [
    "In natural language processing (NLP), a word embedding is a representation of a word. The embedding is used in text analysis. Typically, the representation is a real-valued vector that encodes the meaning of the word in such a way that words that are closer in the vector space are expected to be similar in meaning. Word embeddings can be obtained using language modeling and feature learning techniques, where words or phrases from the vocabulary are mapped to vectors of real numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94604368",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
