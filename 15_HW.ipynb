{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7873a87",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.11.0-cp39-cp39-win_amd64.whl (1.9 kB)\n",
      "Collecting tensorflow-intel==2.11.0\n",
      "  Downloading tensorflow_intel-2.11.0-cp39-cp39-win_amd64.whl (266.3 MB)\n",
      "     -------------------------------------- 266.3/266.3 MB 7.6 MB/s eta 0:00:00\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     ---------------------------------------- 65.5/65.5 KB ? eta 0:00:00\n",
      "Collecting keras<2.12,>=2.11.0\n",
      "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 21.2 MB/s eta 0:00:00\n",
      "Collecting typing-extensions>=3.6.6\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "     ------------------------------------- 439.2/439.2 KB 26.8 MB/s eta 0:00:00\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-15.0.6.1-py2.py3-none-win_amd64.whl (23.2 MB)\n",
      "     --------------------------------------- 23.2/23.2 MB 17.7 MB/s eta 0:00:00\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\a.saratovtsev\\hse\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.24.1)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.51.3-cp39-cp39-win_amd64.whl (3.7 MB)\n",
      "     ---------------------------------------- 3.7/3.7 MB 20.0 MB/s eta 0:00:00\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "     -------------------------------------- 126.5/126.5 KB 7.3 MB/s eta 0:00:00\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp39-cp39-win_amd64.whl (895 kB)\n",
      "     ------------------------------------- 895.9/895.9 KB 18.8 MB/s eta 0:00:00\n",
      "Collecting tensorboard<2.12,>=2.11\n",
      "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "     ---------------------------------------- 6.0/6.0 MB 14.7 MB/s eta 0:00:00\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-23.3.3-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\a.saratovtsev\\hse\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (58.1.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\a.saratovtsev\\hse\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (21.3)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.8.0-cp39-cp39-win_amd64.whl (2.6 MB)\n",
      "     ---------------------------------------- 2.6/2.6 MB 15.4 MB/s eta 0:00:00\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.15.0-cp39-cp39-win_amd64.whl (36 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     ---------------------------------------- 57.5/57.5 KB 3.0 MB/s eta 0:00:00\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\a.saratovtsev\\hse\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 13.4 MB/s eta 0:00:00\n",
      "Collecting wheel<1.0,>=0.23.0\n",
      "  Downloading wheel-0.40.0-py3-none-any.whl (64 kB)\n",
      "     ---------------------------------------- 64.5/64.5 KB 3.4 MB/s eta 0:00:00\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "     ---------------------------------------- 93.3/93.3 KB ? eta 0:00:00\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
      "     ---------------------------------------- 62.8/62.8 KB ? eta 0:00:00\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "     ------------------------------------- 781.3/781.3 KB 16.4 MB/s eta 0:00:00\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.16.2-py2.py3-none-any.whl (177 kB)\n",
      "     ---------------------------------------- 177.2/177.2 KB ? eta 0:00:00\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
      "     ------------------------------------- 233.6/233.6 KB 14.9 MB/s eta 0:00:00\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\a.saratovtsev\\hse\\venv\\lib\\site-packages (from packaging->tensorflow-intel==2.11.0->tensorflow) (3.0.9)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "     -------------------------------------- 155.3/155.3 KB 9.1 MB/s eta 0:00:00\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\a.saratovtsev\\hse\\venv\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (5.0.0)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
      "     ---------------------------------------- 140.9/140.9 KB ? eta 0:00:00\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "     -------------------------------------- 155.3/155.3 KB 9.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\a.saratovtsev\\hse\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.4)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.1.0-cp39-cp39-win_amd64.whl (97 kB)\n",
      "     ---------------------------------------- 97.1/97.1 KB ? eta 0:00:00\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\a.saratovtsev\\hse\\venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\a.saratovtsev\\hse\\venv\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.10.0)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "     ---------------------------------------- 77.1/77.1 KB ? eta 0:00:00\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     ---------------------------------------- 151.7/151.7 KB ? eta 0:00:00\n",
      "Installing collected packages: tensorboard-plugin-wit, pyasn1, libclang, flatbuffers, wrapt, wheel, werkzeug, urllib3, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, rsa, pyasn1-modules, protobuf, opt-einsum, oauthlib, keras, h5py, grpcio, google-pasta, gast, charset-normalizer, certifi, cachetools, absl-py, requests, markdown, google-auth, astunparse, requests-oauthlib, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.0 certifi-2022.12.7 charset-normalizer-3.1.0 flatbuffers-23.3.3 gast-0.4.0 google-auth-2.16.2 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.51.3 h5py-3.8.0 keras-2.11.0 libclang-15.0.6.1 markdown-3.4.1 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-3.19.6 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.28.2 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-intel-2.11.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.2.0 typing-extensions-4.5.0 urllib3-1.26.15 werkzeug-2.2.3 wheel-0.40.0 wrapt-1.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\a.saratovtsev\\hse\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38c0a58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36ee7f22",
   "metadata": {},
   "source": [
    "1\\. Describe in writing why cross entropy performs better as a loss function then mean squared error.\n",
    "\n",
    "2\\. Describe in writing why dropout allows to reduce overfitting.\n",
    "\n",
    "3\\. Using only dense layers and dropouts try to improve performance of the network predicting handwritten digits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59432fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# This initialization code is required due to an error \n",
    "# \"NotFoundError: No algorithm worked\"\n",
    "# when using Conv2D\n",
    "# Probabliy due to problems with cuda 11.\n",
    "# Remove this when fixed\n",
    "# https://github.com/tensorflow/tensorflow/issues/43174\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "from tensorflow.keras.utils import to_categorical as to_categ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f935e506",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a.saratovtsev\\hse\\venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1769: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8984aff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image labels example:  [5 0 4 1 9 2 1 3 1 4 3 5 3 6 1 7 2 8 6 9]\n"
     ]
    }
   ],
   "source": [
    "# Returns tuple of numpy arrays: (x_train, y_train), (x_test, y_test)\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "print(\"Image labels example: \", y_train[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eab44572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train  (12665, 28, 28)\n",
      "test  (2115, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def extract01(X_data, y_data):\n",
    "    \"\"\"Extract images with 0 or 1 only\"\"\"\n",
    "    X_data01 = []\n",
    "    y_data01 = []\n",
    "    for X, y in zip(X_data, y_data):\n",
    "        if y <= 1:\n",
    "            X_data01.append(X)\n",
    "            y_data01.append(y)\n",
    "    return np.array(X_data01), np.array(y_data01)\n",
    "\n",
    "X_train01, y_train01 = extract01(X_train, y_train)\n",
    "X_test01, y_test01 = extract01(X_test, y_test)\n",
    "\n",
    "print(\"train \", X_train01.shape)\n",
    "print(\"test \", X_test01.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0892df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lambda_1 (Lambda)           (None, 28, 28)            0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 84)                65940     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 84)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 56)                4760      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 56)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 28)                1596      \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 28)                0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 28)                812       \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 28)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 10)                290       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 73,398\n",
      "Trainable params: 73,398\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "dropout_rate = 0.02\n",
    "\n",
    "model_mnist_C = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Lambda(lambda data: data / 256.0, input_shape=input_shape),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(input_shape[0]*3, activation='relu'), tf.keras.layers.Dropout(rate=dropout_rate),\n",
    "    tf.keras.layers.Dense(input_shape[0]*2, activation='relu'), tf.keras.layers.Dropout(rate=dropout_rate),\n",
    "    tf.keras.layers.Dense(input_shape[0], activation='relu'), tf.keras.layers.Dropout(rate=dropout_rate),\n",
    "    tf.keras.layers.Dense(input_shape[0], activation='relu'), tf.keras.layers.Dropout(rate=dropout_rate),\n",
    "    tf.keras.layers.Dense(10) \n",
    "])\n",
    "\n",
    "model_mnist_C.compile(optimizer=tf.keras.optimizers.Adam(),  \n",
    "                      loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "model_mnist_C.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61c7d1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1500/1500 - 5s - loss: 0.3869 - accuracy: 0.8814 - val_loss: 0.1657 - val_accuracy: 0.9529 - 5s/epoch - 3ms/step\n",
      "Epoch 2/20\n",
      "1500/1500 - 3s - loss: 0.1562 - accuracy: 0.9537 - val_loss: 0.1234 - val_accuracy: 0.9634 - 3s/epoch - 2ms/step\n",
      "Epoch 3/20\n",
      "1500/1500 - 3s - loss: 0.1169 - accuracy: 0.9651 - val_loss: 0.1090 - val_accuracy: 0.9687 - 3s/epoch - 2ms/step\n",
      "Epoch 4/20\n",
      "1500/1500 - 3s - loss: 0.0931 - accuracy: 0.9711 - val_loss: 0.1122 - val_accuracy: 0.9705 - 3s/epoch - 2ms/step\n",
      "Epoch 5/20\n",
      "1500/1500 - 3s - loss: 0.0800 - accuracy: 0.9750 - val_loss: 0.1093 - val_accuracy: 0.9695 - 3s/epoch - 2ms/step\n",
      "Epoch 6/20\n",
      "1500/1500 - 3s - loss: 0.0681 - accuracy: 0.9798 - val_loss: 0.1211 - val_accuracy: 0.9669 - 3s/epoch - 2ms/step\n",
      "Epoch 7/20\n",
      "1500/1500 - 3s - loss: 0.0614 - accuracy: 0.9816 - val_loss: 0.1206 - val_accuracy: 0.9677 - 3s/epoch - 2ms/step\n",
      "Epoch 8/20\n",
      "1500/1500 - 3s - loss: 0.0570 - accuracy: 0.9818 - val_loss: 0.1133 - val_accuracy: 0.9709 - 3s/epoch - 2ms/step\n",
      "Epoch 9/20\n",
      "1500/1500 - 3s - loss: 0.0509 - accuracy: 0.9840 - val_loss: 0.1084 - val_accuracy: 0.9729 - 3s/epoch - 2ms/step\n",
      "Epoch 10/20\n",
      "1500/1500 - 3s - loss: 0.0452 - accuracy: 0.9853 - val_loss: 0.1008 - val_accuracy: 0.9754 - 3s/epoch - 2ms/step\n",
      "Epoch 11/20\n",
      "1500/1500 - 3s - loss: 0.0423 - accuracy: 0.9868 - val_loss: 0.1087 - val_accuracy: 0.9761 - 3s/epoch - 2ms/step\n",
      "Epoch 12/20\n",
      "1500/1500 - 3s - loss: 0.0400 - accuracy: 0.9873 - val_loss: 0.1117 - val_accuracy: 0.9721 - 3s/epoch - 2ms/step\n",
      "Epoch 13/20\n",
      "1500/1500 - 3s - loss: 0.0404 - accuracy: 0.9871 - val_loss: 0.1215 - val_accuracy: 0.9684 - 3s/epoch - 2ms/step\n",
      "Epoch 14/20\n",
      "1500/1500 - 3s - loss: 0.0342 - accuracy: 0.9890 - val_loss: 0.1145 - val_accuracy: 0.9731 - 3s/epoch - 2ms/step\n",
      "Epoch 15/20\n",
      "1500/1500 - 3s - loss: 0.0333 - accuracy: 0.9898 - val_loss: 0.1184 - val_accuracy: 0.9736 - 3s/epoch - 2ms/step\n",
      "Epoch 16/20\n",
      "1500/1500 - 3s - loss: 0.0322 - accuracy: 0.9895 - val_loss: 0.1071 - val_accuracy: 0.9737 - 3s/epoch - 2ms/step\n",
      "Epoch 17/20\n",
      "1500/1500 - 3s - loss: 0.0297 - accuracy: 0.9904 - val_loss: 0.1179 - val_accuracy: 0.9731 - 3s/epoch - 2ms/step\n",
      "Epoch 18/20\n",
      "1500/1500 - 3s - loss: 0.0294 - accuracy: 0.9908 - val_loss: 0.1151 - val_accuracy: 0.9721 - 3s/epoch - 2ms/step\n",
      "Epoch 19/20\n",
      "1500/1500 - 3s - loss: 0.0274 - accuracy: 0.9913 - val_loss: 0.1152 - val_accuracy: 0.9746 - 3s/epoch - 2ms/step\n",
      "Epoch 20/20\n",
      "1500/1500 - 3s - loss: 0.0266 - accuracy: 0.9917 - val_loss: 0.1195 - val_accuracy: 0.9734 - 3s/epoch - 2ms/step\n"
     ]
    }
   ],
   "source": [
    "hist_mnist_C = model_mnist_C.fit(X_train, to_categ(y_train), epochs=20, validation_split=0.2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2066999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1085 - accuracy: 0.9742\n",
      "acc=0.9742000102996826, loss=0.10846102237701416\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model_mnist_C.evaluate(X_test, to_categ(y_test))\n",
    "print(f\"acc={acc}, loss={loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c973e6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lambda_2 (Lambda)           (None, 28, 28)            0         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 28, 28, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 28, 28, 4)         68        \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 9, 9, 4)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 324)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 56)                18200     \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 56)                0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 10)                570       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,838\n",
      "Trainable params: 18,838\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "dropout_rate = 0.2\n",
    "\n",
    "model_mnist_D = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Lambda(lambda data: data / 256.0, input_shape=input_shape),\n",
    "    tf.keras.layers.Reshape((*input_shape, 1)),\n",
    "    tf.keras.layers.Conv2D(4, (4, 4), padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(3, 3)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(input_shape[0]*2, activation='relu'), \n",
    "    tf.keras.layers.Dropout(rate=dropout_rate), \n",
    "    tf.keras.layers.Dense(10) \n",
    "])\n",
    "\n",
    "model_mnist_D.compile(optimizer=tf.keras.optimizers.Adam(),  \n",
    "                      loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "model_mnist_D.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a2f0119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1500/1500 - 9s - loss: 0.4328 - accuracy: 0.8655 - val_loss: 0.1522 - val_accuracy: 0.9551 - 9s/epoch - 6ms/step\n",
      "Epoch 2/15\n",
      "1500/1500 - 8s - loss: 0.1748 - accuracy: 0.9468 - val_loss: 0.1080 - val_accuracy: 0.9682 - 8s/epoch - 5ms/step\n",
      "Epoch 3/15\n",
      "1500/1500 - 8s - loss: 0.1306 - accuracy: 0.9598 - val_loss: 0.0916 - val_accuracy: 0.9721 - 8s/epoch - 6ms/step\n",
      "Epoch 4/15\n",
      "1500/1500 - 8s - loss: 0.1092 - accuracy: 0.9665 - val_loss: 0.0799 - val_accuracy: 0.9756 - 8s/epoch - 5ms/step\n",
      "Epoch 5/15\n",
      "1500/1500 - 8s - loss: 0.0934 - accuracy: 0.9709 - val_loss: 0.0698 - val_accuracy: 0.9787 - 8s/epoch - 5ms/step\n",
      "Epoch 6/15\n",
      "1500/1500 - 9s - loss: 0.0846 - accuracy: 0.9738 - val_loss: 0.0704 - val_accuracy: 0.9779 - 9s/epoch - 6ms/step\n",
      "Epoch 7/15\n",
      "1500/1500 - 10s - loss: 0.0773 - accuracy: 0.9753 - val_loss: 0.0689 - val_accuracy: 0.9787 - 10s/epoch - 7ms/step\n",
      "Epoch 8/15\n",
      "1500/1500 - 10s - loss: 0.0709 - accuracy: 0.9780 - val_loss: 0.0612 - val_accuracy: 0.9817 - 10s/epoch - 7ms/step\n",
      "Epoch 9/15\n",
      "1500/1500 - 10s - loss: 0.0655 - accuracy: 0.9795 - val_loss: 0.0662 - val_accuracy: 0.9799 - 10s/epoch - 7ms/step\n",
      "Epoch 10/15\n",
      "1500/1500 - 10s - loss: 0.0592 - accuracy: 0.9817 - val_loss: 0.0589 - val_accuracy: 0.9831 - 10s/epoch - 7ms/step\n",
      "Epoch 11/15\n",
      "1500/1500 - 10s - loss: 0.0569 - accuracy: 0.9817 - val_loss: 0.0615 - val_accuracy: 0.9818 - 10s/epoch - 7ms/step\n",
      "Epoch 12/15\n",
      "1500/1500 - 10s - loss: 0.0518 - accuracy: 0.9832 - val_loss: 0.0592 - val_accuracy: 0.9830 - 10s/epoch - 7ms/step\n",
      "Epoch 13/15\n",
      "1500/1500 - 10s - loss: 0.0530 - accuracy: 0.9824 - val_loss: 0.0590 - val_accuracy: 0.9832 - 10s/epoch - 7ms/step\n",
      "Epoch 14/15\n",
      "1500/1500 - 10s - loss: 0.0489 - accuracy: 0.9840 - val_loss: 0.0589 - val_accuracy: 0.9841 - 10s/epoch - 7ms/step\n",
      "Epoch 15/15\n",
      "1500/1500 - 11s - loss: 0.0473 - accuracy: 0.9845 - val_loss: 0.0551 - val_accuracy: 0.9844 - 11s/epoch - 7ms/step\n"
     ]
    }
   ],
   "source": [
    "hist_mnist_D = model_mnist_D.fit(X_train, to_categ(y_train), epochs=15, validation_split=0.2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9602cfd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0490 - accuracy: 0.9841\n",
      "acc=0.9840999841690063, loss=0.04902346059679985\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model_mnist_D.evaluate(X_test, to_categ(y_test))\n",
    "print(f\"acc={acc}, loss={loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "164636c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 113s 1us/step\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras import datasets, layers, models\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f13f6f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the pixels data to float type\n",
    "train_images = train_images.astype('float32')\n",
    "test_images = test_images.astype('float32')\n",
    " \n",
    "# Standardizing (255 is the total number of pixels an image can have)\n",
    "train_images = train_images / 255\n",
    "test_images = test_images / 255 \n",
    "\n",
    "# One hot encoding the target class (labels)\n",
    "num_classes = 10\n",
    "train_labels = np_utils.to_categorical(train_labels, num_classes)\n",
    "test_labels = np_utils.to_categorical(test_labels, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be577b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 32, 32, 32)       128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 32, 32, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 16, 16, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 16, 16, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 16, 16, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 8, 8, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 4, 4, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 128)               262272    \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 552,874\n",
      "Trainable params: 551,722\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(32, (3,3), padding='same', activation='relu', input_shape=(32,32,3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(32, (3,3), padding='same', activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(layers.Dropout(0.3))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))    # num_classes = 10\n",
    "\n",
    "# Checking the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83f5c4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 185s 232ms/step - loss: 1.7216 - accuracy: 0.4054 - val_loss: 1.2336 - val_accuracy: 0.5581\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 185s 237ms/step - loss: 1.1672 - accuracy: 0.5835 - val_loss: 1.0465 - val_accuracy: 0.6352\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 185s 237ms/step - loss: 1.0032 - accuracy: 0.6476 - val_loss: 1.0106 - val_accuracy: 0.6394\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 184s 235ms/step - loss: 0.8966 - accuracy: 0.6874 - val_loss: 0.7717 - val_accuracy: 0.7361\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 174s 223ms/step - loss: 0.8152 - accuracy: 0.7160 - val_loss: 0.7164 - val_accuracy: 0.7439\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 172s 220ms/step - loss: 0.7637 - accuracy: 0.7374 - val_loss: 0.6683 - val_accuracy: 0.7671\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 172s 219ms/step - loss: 0.7209 - accuracy: 0.7507 - val_loss: 0.6766 - val_accuracy: 0.7671\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 171s 219ms/step - loss: 0.6816 - accuracy: 0.7637 - val_loss: 0.6276 - val_accuracy: 0.7824\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 170s 218ms/step - loss: 0.6533 - accuracy: 0.7744 - val_loss: 0.6406 - val_accuracy: 0.7798\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 171s 219ms/step - loss: 0.6261 - accuracy: 0.7849 - val_loss: 0.5701 - val_accuracy: 0.8076\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_images, train_labels, batch_size=64, epochs=10,\n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "706241fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 7s 23ms/step - loss: 0.5701 - accuracy: 0.8076\n",
      "acc=0.8076000213623047, loss=0.5701103806495667\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(test_images, test_labels)\n",
    "print(f\"acc={acc}, loss={loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd157a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82b002e4",
   "metadata": {},
   "source": [
    "Dropout regularization will ensure the following:\n",
    "\n",
    "- The neurons canâ€™t rely on one input because it might be dropped out at random. This reduces bias due to over-relying on one input, bias is a major cause of overfitting.\n",
    "\n",
    "- Neurons will not learn redundant details of inputs. This ensures only important information is stored by the neurons. This enables the neural network to gain useful knowledge which it uses to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac99e67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a390c9e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a824f3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c96c22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfeee77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584fa802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee7fa37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d01760",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
